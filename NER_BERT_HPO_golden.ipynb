{"cells":[{"cell_type":"code","execution_count":12,"id":"p7QH2OAexwh5","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1756252593929,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"p7QH2OAexwh5"},"outputs":[],"source":["# !pip install seqeval"]},{"cell_type":"code","execution_count":13,"id":"e-yOTRURAZ72","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1756252593932,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"e-yOTRURAZ72"},"outputs":[],"source":["# !pip install evaluate"]},{"cell_type":"code","execution_count":14,"id":"55ViNYK1GMg-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":942,"status":"ok","timestamp":1756252594876,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"55ViNYK1GMg-","outputId":"c7783836-c97e-4d85-ed63-247f9a17730e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","base_path = '/content/drive/MyDrive/NLP/'"]},{"cell_type":"code","execution_count":15,"id":"k1xIn_5iHfTm","metadata":{"id":"k1xIn_5iHfTm","executionInfo":{"status":"ok","timestamp":1756252594877,"user_tz":-480,"elapsed":4,"user":{"displayName":"宋若冰","userId":"16843204755956256636"}}},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":16,"id":"fc9e92bf-1159-40e4-b3c4-17a94345cafc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":797,"status":"ok","timestamp":1756252595672,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"fc9e92bf-1159-40e4-b3c4-17a94345cafc","outputId":"5c27ffc8-dbcb-4516-a38d-72e125404296"},"outputs":[{"output_type":"stream","name":"stdout","text":["7 types of entities are discovered:\n","- PATIENT: 1000 samples\n","- HPO_TERM: 1000 samples\n","- AGE_ONSET: 1000 samples\n","- AGE_FOLLOWUP: 1000 samples\n","- AGE_DEATH: 1000 samples\n","- GENE: 1000 samples\n","- GENE_VARIANT: 1000 samples\n","7 types of entities are discovered:\n","- AGE_ONSET: 93 samples\n","- PATIENT: 246 samples\n","- HPO_TERM: 2525 samples\n","- GENE: 252 samples\n","- GENE_VARIANT: 404 samples\n","- AGE_FOLLOWUP: 76 samples\n","- AGE_DEATH: 29 samples\n"]}],"source":["import json\n","from collections import defaultdict\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import TrainingArguments, Trainer\n","from transformers import EarlyStoppingCallback\n","from sklearn.metrics import classification_report as sk_classification_report\n","from sklearn.metrics import accuracy_score\n","from datasets import Dataset\n","import torch\n","import random\n","from collections import Counter\n","from transformers import set_seed\n","\n","SEED = 42\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","set_seed(SEED)\n","\n","def convert_to_iob(entries):\n","    samples = []\n","    label_map = defaultdict(list)\n","\n","    for entry in entries:\n","        tokens = entry[\"tokens\"]\n","        text = entry[\"text\"]\n","        spans = entry.get(\"spans\", [])\n","\n","        labels = [\"O\"] * len(tokens)\n","\n","        for span in spans:\n","            start_token = span[\"token_start\"]\n","            end_token = span[\"token_end\"]\n","            label = span[\"label\"]\n","\n","            # BIO Tagging\n","            if label in [\"HPO_TERM\"]:\n","                labels[start_token] = f\"B-{label}\"\n","                for i in range(start_token + 1, end_token + 1):\n","                    labels[i] = f\"I-{label}\"\n","\n","            label_map[label].append(span)\n","\n","        token_label_pairs = [\n","            (token[\"text\"], label)\n","            for token, label in zip(tokens, labels)\n","        ]\n","        samples.append(token_label_pairs)\n","\n","    return samples, label_map\n","\n","entries_silver = []\n","entries_golden = []\n","with open(base_path+\"dataset/synthesis/silver.jsonl\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        entries_silver.append(json.loads(line))\n","\n","with open(base_path+\"dataset/cleaned/NER/processed_merged.jsonl\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        entries_golden.append(json.loads(line))\n","\n","silver_data, label_stats_silver = convert_to_iob(entries_silver)\n","golden_data, label_stats_golden = convert_to_iob(entries_golden)\n","\n","# Printed label statistics\n","print(f\"{len(label_stats_silver)} types of entities are discovered:\")\n","for label, spans in label_stats_silver.items():\n","    print(f\"- {label}: {len(spans)} samples\")\n","\n","print(f\"{len(label_stats_golden)} types of entities are discovered:\")\n","for label, spans in label_stats_golden.items():\n","    print(f\"- {label}: {len(spans)} samples\")"]},{"cell_type":"code","execution_count":17,"id":"fe58704b-22e6-4be8-a758-ac7a6b93ffee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2448,"status":"ok","timestamp":1756252598121,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"fe58704b-22e6-4be8-a758-ac7a6b93ffee","outputId":"7b3c36a6-f6c7-4ca2-c76b-1483c8b3f301"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# label_list = [\"O\"] + [\n","#     f\"{pre}-{label}\"\n","#     for label in label_stats.keys()\n","#     for pre in [\"B\", \"I\"]\n","# ]\n","\n","label_list = ['O', 'B-HPO_TERM', 'I-HPO_TERM']\n","\n","# model_name = \"bert-base-cased\"\n","# model_name = \"allenai/scibert_scivocab_uncased\"\n","# model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n","model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n","# model_name = \"prajjwal1/bert-tiny\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(label_list),\n","    id2label={i: label for i, label in enumerate(label_list)},\n","    label2id={label: i for i, label in enumerate(label_list)}\n",")\n"]},{"cell_type":"code","execution_count":18,"id":"771a7c09-7e27-40c9-85d9-76ca87aab225","metadata":{"id":"771a7c09-7e27-40c9-85d9-76ca87aab225","executionInfo":{"status":"ok","timestamp":1756252598772,"user_tz":-480,"elapsed":648,"user":{"displayName":"宋若冰","userId":"16843204755956256636"}}},"outputs":[],"source":["def encode_data(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[\"tokens\"],\n","        truncation=True,\n","        is_split_into_words=True,\n","        padding=\"max_length\",\n","        max_length=256\n","    )\n","\n","    label = examples[\"labels\"]\n","    word_ids = tokenized_inputs.word_ids()\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","        if word_idx is None:\n","            label_ids.append(-100)\n","        elif word_idx != previous_word_idx:\n","            label_ids.append(label[word_idx])\n","        else:\n","            label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    tokenized_inputs[\"labels\"] = label_ids\n","    return tokenized_inputs\n","\n","\n","formatted_data_silver = [{\"tokens\": [t[0] for t in sample], \"labels\": [label_list.index(t[1]) for t in sample]}\n","                 for sample in silver_data]\n","formatted_data_golden = [{\"tokens\": [t[0] for t in sample], \"labels\": [label_list.index(t[1]) for t in sample]}\n","                 for sample in golden_data]\n","\n","from sklearn.model_selection import train_test_split\n","train_set, temp_set = train_test_split(formatted_data_golden, test_size=0.3, random_state=SEED)\n","val_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=SEED)\n","\n","encoded_train = [encode_data(d) for d in train_set]\n","encoded_val = [encode_data(d) for d in val_set]\n","encoded_test = [encode_data(d) for d in test_set]"]},{"cell_type":"code","execution_count":19,"id":"3a7c2876-30cc-496a-839e-c0ca003da03f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":290485,"status":"ok","timestamp":1756252889260,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"3a7c2876-30cc-496a-839e-c0ca003da03f","outputId":"193c47dd-5ccc-48a8-ace4-c140813b07e0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3682181766.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='456' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 456/1140 04:48 < 07:15, 1.57 it/s, Epoch 8/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Strict Precision</th>\n","      <th>Strict Recall</th>\n","      <th>Strict F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.530600</td>\n","      <td>0.206798</td>\n","      <td>0.746011</td>\n","      <td>0.488676</td>\n","      <td>0.590526</td>\n","      <td>0.331050</td>\n","      <td>0.362500</td>\n","      <td>0.346062</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.167100</td>\n","      <td>0.179870</td>\n","      <td>0.654971</td>\n","      <td>0.878049</td>\n","      <td>0.750279</td>\n","      <td>0.444079</td>\n","      <td>0.675000</td>\n","      <td>0.535714</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.107600</td>\n","      <td>0.161223</td>\n","      <td>0.824635</td>\n","      <td>0.688153</td>\n","      <td>0.750237</td>\n","      <td>0.543124</td>\n","      <td>0.582500</td>\n","      <td>0.562123</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.066600</td>\n","      <td>0.168235</td>\n","      <td>0.754606</td>\n","      <td>0.784843</td>\n","      <td>0.769428</td>\n","      <td>0.493151</td>\n","      <td>0.630000</td>\n","      <td>0.553238</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.040000</td>\n","      <td>0.200509</td>\n","      <td>0.758117</td>\n","      <td>0.813589</td>\n","      <td>0.784874</td>\n","      <td>0.562887</td>\n","      <td>0.682500</td>\n","      <td>0.616949</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.025800</td>\n","      <td>0.209558</td>\n","      <td>0.796820</td>\n","      <td>0.785714</td>\n","      <td>0.791228</td>\n","      <td>0.620225</td>\n","      <td>0.690000</td>\n","      <td>0.653254</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.018600</td>\n","      <td>0.255893</td>\n","      <td>0.808630</td>\n","      <td>0.750871</td>\n","      <td>0.778681</td>\n","      <td>0.610837</td>\n","      <td>0.620000</td>\n","      <td>0.615385</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.012600</td>\n","      <td>0.271010</td>\n","      <td>0.802218</td>\n","      <td>0.756098</td>\n","      <td>0.778475</td>\n","      <td>0.567873</td>\n","      <td>0.627500</td>\n","      <td>0.596200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=456, training_loss=0.12112626354945333, metrics={'train_runtime': 289.3407, 'train_samples_per_second': 31.105, 'train_steps_per_second': 3.94, 'total_flos': 470338414387200.0, 'train_loss': 0.12112626354945333, 'epoch': 8.0})"]},"metadata":{},"execution_count":19}],"source":["training_args = TrainingArguments(\n","    output_dir=base_path + \"./model/NER\",\n","    eval_strategy=\"epoch\",\n","    num_train_epochs=20,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    learning_rate=3e-5,\n","    weight_decay=0.05,\n","    warmup_ratio=0.05,\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    seed=SEED,\n","    metric_for_best_model=\"strict_f1\",\n","    save_total_limit=2,\n","    logging_strategy=\"epoch\",\n","    report_to=\"none\",\n",")\n","\n","from evaluate import load\n","seqeval = load(\"seqeval\")\n","\n","from itertools import chain\n","\n","def is_loose_match(true_tag, pred_tag):\n","    if true_tag != \"O\" and pred_tag != \"O\":\n","        return true_tag.split(\"-\")[-1] == pred_tag.split(\"-\")[-1]\n","    return False\n","\n","def strip_prefix(tag):\n","    return tag.split(\"-\")[-1] if tag != \"O\" else \"O\"\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    pred_ids = np.argmax(predictions, axis=2)\n","\n","    true_labels = [[label_list[l] for l in lab if l != -100] for lab in labels]\n","    pred_labels = [[label_list[p] for p, l in zip(pred, lab) if l != -100]\n","            for pred, lab in zip(pred_ids, labels)]\n","\n","    # Strict matching\n","    strict = seqeval.compute(predictions=pred_labels, references=true_labels)\n","    strict_precision = strict[\"overall_precision\"]\n","    strict_recall = strict[\"overall_recall\"]\n","    strict_f1 = strict[\"overall_f1\"]\n","\n","    # Loose matching\n","    flat_true = list(chain.from_iterable(true_labels))\n","    flat_pred = list(chain.from_iterable(pred_labels))\n","\n","    adjusted_pred = [t if is_loose_match(t, p) else p for t, p in zip(flat_true, flat_pred)]\n","\n","    flat_true_no_prefix = [strip_prefix(t) for t in flat_true]\n","    adjusted_pred_no_prefix = [strip_prefix(p) for p in adjusted_pred]\n","\n","    labels_without_O = sorted((set(flat_true_no_prefix) | set(adjusted_pred_no_prefix)) - {\"O\"})\n","\n","    report = sk_classification_report(\n","        flat_true_no_prefix,\n","        adjusted_pred_no_prefix,\n","        labels=labels_without_O,\n","        output_dict=True,\n","        zero_division=0\n","    )\n","\n","    loose_precision = report[\"weighted avg\"][\"precision\"]\n","    loose_recall = report[\"weighted avg\"][\"recall\"]\n","    loose_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","    return {\n","        \"precision\": loose_precision,\n","        \"recall\": loose_recall,\n","        \"f1\": loose_f1,\n","\n","        \"strict_precision\": strict_precision,\n","        \"strict_recall\": strict_recall,\n","        \"strict_f1\": strict_f1\n","    }\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_train,\n","    eval_dataset=encoded_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":20,"id":"S35NVofIzWVg","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":1936,"status":"ok","timestamp":1756252891201,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"S35NVofIzWVg","outputId":"c1fce171-da1b-4b2d-ab9a-ca5356754ea3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}],"source":["dataset_test = encoded_test\n","results = trainer.predict(dataset_test)\n","\n","logits = results.predictions\n","label_ids = results.label_ids\n","\n","pred_indices = np.argmax(logits, axis=-1)\n","\n","true_labels = []\n","pred_labels = []\n","\n","for i in range(len(label_ids)):\n","    true_seq = label_ids[i]\n","    pred_seq = pred_indices[i]\n","\n","    filtered_true = [label_list[l] for l in true_seq if l != -100]\n","    filtered_pred = [label_list[p] for p, l in zip(pred_seq, true_seq) if l != -100]\n","\n","    true_labels.append(filtered_true)\n","    pred_labels.append(filtered_pred)"]},{"cell_type":"code","execution_count":21,"id":"ErQ8eqVbvRDw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1756252891516,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"ErQ8eqVbvRDw","outputId":"29851908-2ddf-459f-c345-70d7116545ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9478\n","Precision: 0.5871\n","Recall: 0.6709\n","F1-Score: 0.6262\n","              precision    recall  f1-score   support\n","\n","    HPO_TERM       0.59      0.67      0.63       392\n","\n","   micro avg       0.59      0.67      0.63       392\n","   macro avg       0.59      0.67      0.63       392\n","weighted avg       0.59      0.67      0.63       392\n","\n"]}],"source":["# Generate classification report\n","from seqeval.metrics import classification_report as seqeval_classification_report\n","report = seqeval_classification_report(true_labels, pred_labels, output_dict=True)\n","\n","from sklearn.metrics import accuracy_score\n","from itertools import chain\n","\n","flat_true = list(chain.from_iterable(true_labels))\n","flat_pred = list(chain.from_iterable(pred_labels))\n","\n","accuracy = accuracy_score(flat_true, flat_pred)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","print(f\"Precision: {report['weighted avg']['precision']:.4f}\")\n","print(f\"Recall: {report['weighted avg']['recall']:.4f}\")\n","print(f\"F1-Score: {report['weighted avg']['f1-score']:.4f}\")\n","\n","print(seqeval_classification_report(true_labels, pred_labels))"]},{"cell_type":"code","execution_count":22,"id":"c71323f4-8bb2-4529-98fe-8a1743581e2c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1756252891843,"user":{"displayName":"宋若冰","userId":"16843204755956256636"},"user_tz":-480},"id":"c71323f4-8bb2-4529-98fe-8a1743581e2c","outputId":"93343c9d-39a3-4eb1-bc41-b7013dda1954"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9522\n","Precision: 0.7748\n","Recall: 0.7795\n","F1-Score: 0.7772\n","              precision    recall  f1-score   support\n","\n","    HPO_TERM       0.77      0.78      0.78      1161\n","\n","   micro avg       0.77      0.78      0.78      1161\n","   macro avg       0.77      0.78      0.78      1161\n","weighted avg       0.77      0.78      0.78      1161\n","\n"]}],"source":["from sklearn.metrics import classification_report as sk_classification_report\n","from itertools import chain\n","\n","# Loose matching\n","def is_loose_match(true_tag, pred_tag):\n","    if true_tag != \"O\" and pred_tag != \"O\":\n","        true_entity = true_tag.split(\"-\")[-1]\n","        pred_entity = pred_tag.split(\"-\")[-1]\n","        return true_entity == pred_entity\n","    return False\n","\n","def strip_prefix(tag):\n","    return tag.split(\"-\")[-1] if tag != \"O\" else \"O\"\n","\n","flat_true = list(chain.from_iterable(true_labels))\n","flat_pred = list(chain.from_iterable(pred_labels))\n","\n","adjusted_pred = []\n","for t, p in zip(flat_true, flat_pred):\n","    if is_loose_match(t, p):\n","        adjusted_pred.append(t)\n","    else:\n","        adjusted_pred.append(p)\n","\n","flat_true_no_prefix = [strip_prefix(t) for t in flat_true]\n","adjusted_pred_no_prefix = [strip_prefix(p) for p in adjusted_pred]\n","\n","labels_without_O = sorted((set(flat_true_no_prefix) | set(adjusted_pred_no_prefix)) - {\"O\"})\n","\n","accuracy = accuracy_score(flat_true_no_prefix, adjusted_pred_no_prefix)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","report = sk_classification_report(\n","    flat_true_no_prefix,\n","    adjusted_pred_no_prefix,\n","    labels=labels_without_O,\n","    output_dict=True,\n","    zero_division=0\n",")\n","\n","print(f\"Precision: {report['weighted avg']['precision']:.4f}\")\n","print(f\"Recall: {report['weighted avg']['recall']:.4f}\")\n","print(f\"F1-Score: {report['weighted avg']['f1-score']:.4f}\")\n","\n","print(sk_classification_report(\n","    flat_true_no_prefix,\n","    adjusted_pred_no_prefix,\n","    labels=labels_without_O,\n","    zero_division=0\n","))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python [conda env:nlp_env]","language":"python","name":"conda-env-nlp_env-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.23"}},"nbformat":4,"nbformat_minor":5}